# Wikipedia Text Similarity Recommender 

## Abstract
We look into the problem of generating  a Wikipedia article recommendation system. Wikipedia currently does not have a recommendation system. It relies on internal reference links to keep users engaged. We want to build a recommendation system for extracting relevant articles based on text similarity analysis. 

### 1	Introduction
Our project started with the idea to build a recommender system by performing text similarity analysis on a sufficiently large dataset. Given the large scale we anticipate our data being, we will take advantage of parallel processing in Apache Spark. The dataset that we chose is the English Wikipedia collection, which is around 58GB in size and includes over 19 million articles. The size of this dataset is ideal for a big data project.  Originally we had concern over the uncertainty of the efficiency of a wikipedia recommender system due to the vast amount of keywords, subjects and articles that span across the model. Since wikipedia does not provide explicit user ratings, we relied on building a model to find a similarity measure between the texts in articles. Through theoretical knowledge that we learned from the class, we utilized tools and APIs such as Apache Spark, Python, Scikit, Numpy, Google Cloud Platform (GCP) in order to collect and distribute the data. For text similarities, the techniques that were used for this project included Cosine Similarity, as well as implementing a pre trained FastText word embeddings model.

### 2	Relevant Work
We have decided to focus on a ranking/recommendation system due to its industry relevance and relation to big data in a real-world application. This project will allow us to better understand how different online web applications (Instagram, Spotify, Youtube, etc..) function, which will prepare us for future work opportunities in the data science field. Recommender systems have become a vital part of content sharing platforms in recent years, and the nature of such systems is variable and dependent on the application. Netflix movie recommendations, as well as Youtube rely on ratings, comments, and other user generated features for measuring relevance between items. However, Wikipedia is different than most platforms in one key area. There are no reviews, comments, likes, or any other user generated features. For this reason we focus on text similarity analysis to calculate relevance. To calculate relevance between articles, we use cosine similarity on a pre-trained word2vec embeddings model that includes 1 million word vectors, with 300 dimensions per vector. 

### 3 Implementation
3.1 The Data
We are working with the English Wikipedia collection. This was available for download as a 15GB zip file that decompresses to just over 58GB [1]. We put this directly into our Google Cloud Storage Bucket. From this xml file we extracted Title, Id, and the text. 
3.2 The Cluster
As a group we decided that using the Google Cloud Platform would be the best platform to process our data and perform text similarity analysis. GCP has a tool called Dataproc that creates a spark cluster. Since we are working with a large dataset and creating a matrix of all of its elements, we knew to provision a cluster with as much RAM as possible. Our final configuration had 1 master node with 2 CPUs and 13.0 GB memory and 4 worker nodes with 4 CPUs and 26.0 GB memory each. 
3.3 The Code
We wrote all of our code in Python. We did this because we knew it would have all of the libraries we would need for text similarity analysis as well as a Spark API. The Wikipedia data we downloaded was stored in xml format so we needed to use a 3rd party library created by DataBricks to create a dataframe [2]. To do this we downloaded a jar file of the library and stored it in our bucket for easy reference while submitting a spark job. We also had to make sure that all of the conda and python libraries we used were included when provisioning the cluster. After loading the data into a DataFrame, used spark to transform our data. We changed the text column from a single string to an array of strings. From there we remove non-alphanumeric characters and stop words using pysaprk’s StopWordsRemover. We also kept the title and a unique ID for each article. Now we are ready for calculating text similarity.
3.4 Text Similarity
For calculating the text similarities between articles, we attempted to utilize two different approaches while the following libraries: gensim word2vec and softcossim, corpora dictionary, and scipy spatial. For the first approach, we instantiated a dictionary for all unique words in the dataset, a similarity word embeddings matrix, and a bag of words for each article. created a corpora dictionary assimilating all the unique words in all articles. Corpora is used to create a dictionary, which hashes all the unique words to indices that can be easily referenced to find the associated word embedding vectors. For the word embeddings, we created a gensim word2vec model that used the FastText pre-trained model of 1 million 300 dimensional word vectors trained with subword information on Wikipedia 2017 [3]. Afterwards, we create bag of words for each article using the doc2bow function to reference the index of words in the corpora dictionary. To calculate the similarity between articles, we used the gensim softcossim function on two target articles while referencing the similarity matrix model to measure the distance between word vectors. Although this approach works on small data sets, we quickly encountered a memory error when attempted to run it on the large Wikipedia dataset. Our hypothesis was that this error was due to the gensim model not able to parallel process using Apache Spark, thus we improvised by implementing a different approach that utilized RDDs instead. For this second approach, we created an RDD word embeddings model, calculated the “shape” of each article, then calculated the relevance by measuring the cosine similarity between article “shapes”.The word embedding model is uniform for all applications. For this reason we were able to load it from a .vec file into an RDD. From there we were able to manipulate the model such that each element has two attributes: word and vector. To calculate the “shape” for each article, we looped through each word and mapped the word embeddings model RDD to filter the corresponding word vector, then summed all the word vectors. To calculate the similarity between article “shapes”, we used the scipy spatial function. However, although this approach did not result in a memory error, the run time was too large to calculate the recommendations in a timely manner. This large run time complexity is due to the filter function used when mapping the word embeddings model, which needs to filter through 1 million words to find the associated word vector. Finally, the output of both similarity approaches would be a recommendation matrix that includes rows for each article, and ordered columns for each article recommendation. The size of the matrix would be nxn, with n being the number of articles in the dataset. Given that our dataset includes 19 million articles, the recommendation matrix would include an astonishing 3.61e14 entries. To solve this large space complexity problem, we looped through each article, calculated the cosine similarities, sorted the similarities in descending order, and only included the top 10 similarities. 

### 4	Results 
We successfully preprocessed our data using Spark. Through DataFrame and RDD transformations we got to the point where the text similarity algorithm could be applied. However, this is where we started to experience issues. Due to the large scale of the data and the need to look at every individual word, the runtime of the text similarity portion was too long and therefore too expensive to run on the cluster. Each article took approximately one hour to calculate its Vector.  This process taught us a lot. We learned to underestimate how long it will take to process the data before text analysis can be applied. Furthermore, we gained an insight on the challenges of calculating text similarity while using a word embeddings model on large datasets. 

### 5	Further Work
Our code is very inefficient. Generating the similarity matrix is very expensive. In the future we would look for an alternative algorithm or a spark alternative. We could also fix this bottleneck by gaining access to more nodes with more memory. This would reduce runtimes and possibly make our outcome cost efficient. Lastly, we want to look into how this could actually be implemented in Wikipedia or through a browser plugin.

### 6	Conclusion
With enough resources any big data problem can be considered small enough for a given group to tackle, however, when the volume of data is too large for the resources at hand the job cannot be completed. In our case, building a model to analyse text based similarity across all of Wikipedia proved to be too big a job. We believe given enough resources to build the model, i.e. a bigger cluster and more time, we could have successfully processed the contents of Wikipedia into article recommendations.



### 7	References
[1] WikiMedia “Data dump torrents” 31 July 2019. Available: https://meta.wikimedia.org/wiki/Data_dump_torrents#English_Wikipedia

[2] DataBricks “Databricks/spark-xml” Available:https://github.com/databricks/spark-xml

[3]  Facebook Inc “English word vectors” Available:https://fasttext.cc/docs/en/english-vectors.html

