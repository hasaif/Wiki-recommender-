{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split, col, struct, udf\n",
    "from pyspark.sql import Row\n",
    "import sys\n",
    "#import google_compute_engine\n",
    "import gensim\n",
    "from gensim import corpora,models,similarities\n",
    "from gensim.matutils import softcossim \n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Final\")\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## load word embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(\"gs://wiki_final/subword.vec\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(SparkFiles.get(\"subword.vec\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load wiki dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml = spark.read.format('xml').options(rowTag=\"page\").load('gs://wiki_final/big_data.xml.bz2')\n",
    "xml = spark.read.format('xml').options(rowTag=\"page\").load('gs://wiki_final/Wikipedia-test-SUBSET.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used in map function\n",
    "def getText(row):\n",
    "        #complex struct structure to get text field \n",
    "        s = row.revision.text._VALUE\n",
    "        #return text a id(used for join)\n",
    "        return   Row(title =row.title, text= prepText(s) ,id_= row.id)\n",
    "\n",
    "def prepText(s):\n",
    "        punc='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'\n",
    "        lowercased_str = str(s).lower()\n",
    "        for ch in punc:\n",
    "            lowercased_str = lowercased_str.replace(ch, ' ')\n",
    "        return rmSp(lowercased_str.split(' '))\n",
    "                            \n",
    "def rmSp(x):\n",
    "        r = []\n",
    "        for w in x:\n",
    "            if w!='':\n",
    "                r.append(w)\n",
    "        return r\n",
    "\n",
    "#df -> rdd to transform the data into sql Rows and make text array\n",
    "textRDD = xml.select('revision', 'id', 'title').rdd.map(getText)#makes a DF with text and ID \n",
    "#back to df to strip stopwords \n",
    "t = textRDD.toDF()\n",
    "remover = StopWordsRemover(inputCol=\"text\", outputCol=\"filtered\")\n",
    "stop_removed =  remover.transform(t)\n",
    "\n",
    "#New data with id_, title, text, filtered\n",
    "stop_removed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions for splitting data and calculating article vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_shape_fun(article):\n",
    "    article_vector = [0.0] *300\n",
    "    for word in article:\n",
    "        article_vector += model.filter(lambda x: x.word == word).map(lambda x: x.vec).take(1)\n",
    "    return article_vector\n",
    "\n",
    "def str2arr(v):\n",
    "    arr = []\n",
    "    m = v.split(' ')\n",
    "    for n in m[1:]:\n",
    "        n = re.sub('[^0-9]','', n)\n",
    "        arr.append(float(n))\n",
    "    return arr\n",
    "\n",
    "def split_vec(x):\n",
    "    s = ''\n",
    "    v = ''\n",
    "    for l in str(x.encode('utf8')):\n",
    "        if l.isalpha():\n",
    "            s+= l\n",
    "        else:\n",
    "            v += l\n",
    "    return Row(word = s, vec = str2arr(v)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df -> rdd to transform the data into sql Rows and make text array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textRDD = xml.select('revision', 'id', 'title').rdd.map(getText)#makes a DF with text and ID \n",
    "#back to df to strip stopwords \n",
    "t = textRDD.toDF()\n",
    "remover = StopWordsRemover(inputCol=\"text\", outputCol=\"filtered\")\n",
    "stop_removed =  remover.transform(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## partition data to attempt to get some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = stop_removed.rdd.map(lambda x: Row(id_ = x.id_, text = x.filtered)).take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate article vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_shapes = sc.emptyRDD()\n",
    "for article in sub:\n",
    "    article_shape = sc.parallelize(Row(id_= article.id_, shape = article_shape_fun(article.text)))\n",
    "    all_shapes = all_shapes.union(article_shape)\n",
    "    print(article.id_)\n",
    "print(\"shapes done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for each article:\n",
    "* calculate cosine similarities\n",
    "* sort similarities\n",
    "* save top 10 similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over Article shapes and get the top 10 Recomendations\n",
    "all_recs = sc.emptyRDD()\n",
    "for article in all_shapes.collect():\n",
    "    recs = all_shapes.map(lambda x: Row(id_= x.id, sim= 1 - spatial.distance.cosine(article.shape, x.shape))).sortBy(lambda x: x.sim, ascending=False).take(11)\n",
    "    article_recs = sc.parallelize((article.id,recs))\n",
    "    all_recs = all_recs.union(article_recs)\n",
    "\n",
    "\n",
    "#save results RDD to bucket\n",
    "all_recs.saveAsTextFile(\"gs://wiki_final/rec_id\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
